# create_and_save_dataset.py (REVISED)
import torch
import numpy as np
from torch_geometric.data import Data
import os

def create_synthetic_dataset(num_samples, seq_len_range, feature_dim, go_term_count, num_domains):
    """
    创建一个包含多个样本的简单模拟数据集。
    新版：将 domains 和 labels 直接作为 Data 对象的属性。
    """
    dataset = []
    for i in range(num_samples):
        seq_length = np.random.randint(*seq_len_range)
        
        # 1. 创建基础图数据
        node_features = torch.randn(seq_length, feature_dim)
        num_edges = np.random.randint(seq_length, seq_length * 2)
        edge_index = torch.randint(0, seq_length, (2, num_edges))
        graph_data = Data(x=node_features, edge_index=edge_index)
        
        # 2. 创建域ID和标签
        num_protein_domains = np.random.randint(1, 6)
        domain_ids = torch.LongTensor(np.random.randint(0, num_domains, size=num_protein_domains))
        
        num_labels = np.random.randint(5, 21)
        labels = torch.zeros(1, go_term_count) # 保持 [1, C] 的形状
        label_indices = np.random.randint(0, go_term_count, size=num_labels)
        labels[0, label_indices] = 1.0
        
        # 3. 将 domains 和 labels 作为属性附加到 graph_data 对象上
        graph_data.domains = domain_ids
        graph_data.labels = labels
        
        dataset.append(graph_data)
        
    print(f"成功创建 {num_samples} 个 PyG Data 对象。")
    return dataset

if __name__ == '__main__':
    print("--- 1. 开始生成 PyG 格式的模拟数据集 ---")
    
    try:
        go_data = torch.load('data/go_data.pth', weights_only=False)
        GO_TERM_COUNT = len(go_data['go_to_idx'])
    except FileNotFoundError:
        print("错误: data/go_data.pth 未找到。请先运行 prepare_go_dag.py。")
        exit()

    NUM_DOMAINS = 5000
    FEATURE_DIM = 1280
    NUM_TRAIN_SAMPLES = 100
    NUM_VAL_SAMPLES = 20
    SEQ_LEN_RANGE = (100, 300)

    train_dataset = create_synthetic_dataset(NUM_TRAIN_SAMPLES, SEQ_LEN_RANGE, FEATURE_DIM, GO_TERM_COUNT, NUM_DOMAINS)
    val_dataset = create_synthetic_dataset(NUM_VAL_SAMPLES, SEQ_LEN_RANGE, FEATURE_DIM, GO_TERM_COUNT, NUM_DOMAINS)
    
    output_dir = "synthetic_dataset"
    os.makedirs(output_dir, exist_ok=True)
    
    train_path = os.path.join(output_dir, "train_dataset.pth")
    val_path = os.path.join(output_dir, "val_dataset.pth")
    
    # 保存 Data 对象列表
    torch.save(train_dataset, train_path)
    torch.save(val_dataset, val_path)
    
    print(f"\n--- 2. 数据集已成功保存 ---")
    print(f"训练集路径: {train_path}")
    print(f"验证集路径: {val_path}")
# evaluate.py
import torch
from tqdm import tqdm
import numpy as np
from sklearn.metrics import precision_recall_curve, auc

def calculate_metrics(y_true, y_pred_probs):
    """
    计算 Fmax 和 AUPR 指标。
    - y_true: 真实的标签矩阵 [num_samples, num_go_terms]
    - y_pred_probs: 模型预测的概率矩阵 [num_samples, num_go_terms]
    """
    # 将数据移回CPU并转换为numpy数组进行计算
    y_true = y_true.cpu().numpy()
    y_pred_probs = y_pred_probs.cpu().numpy()
    
    # --- 计算 Fmax ---
    fmax = 0.0
    for threshold in np.linspace(0, 1, 101):
        precision_total, recall_total = 0, 0
        num_proteins_with_preds = 0
        for i in range(y_true.shape[0]):
            preds = y_pred_probs[i] > threshold
            true_positives = np.sum(preds & y_true[i].astype(bool))
            if np.sum(preds) > 0:
                num_proteins_with_preds += 1
                precision_total += true_positives / np.sum(preds)
            if np.sum(y_true[i]) > 0:
                recall_total += true_positives / np.sum(y_true[i])
        if num_proteins_with_preds > 0:
            avg_precision = precision_total / num_proteins_with_preds
            avg_recall = recall_total / y_true.shape[0]
            if avg_precision + avg_recall > 0:
                f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall)
                if f1 > fmax: fmax = f1
    
    # --- 计算 AUPR (Macro Average) ---
    aupr_scores = []
    for i in range(y_true.shape[1]):
        if np.sum(y_true[:, i]) > 0:
            precision, recall, _ = precision_recall_curve(y_true[:, i], y_pred_probs[:, i])
            aupr_scores.append(auc(recall, precision))
    macro_aupr = np.mean(aupr_scores) if aupr_scores else 0.0
    
    return fmax, macro_aupr

def evaluate_model(model, data_loader, device):
    """
    在给定的数据集上评估模型。
    此版本已更新，可正确处理 PyTorch Geometric 的批次数据。
    """
    model.eval()
    all_labels = []
    all_preds = []
    with torch.no_grad():
        eval_pbar = tqdm(data_loader, desc="[Validation]")
        for batch_data in eval_pbar:
            # 1. 将整个批次数据一次性移动到 GPU
            batch_data = batch_data.to(device)

            # 2. 将 Batch 对象分解为 Data 对象列表
            graph_list = batch_data.to_data_list()
            
            # 3. 逐个处理批次中的每个图
            for graph in graph_list:
                probs, _ = model(graph, graph.domains)
                all_preds.append(probs)
                all_labels.append(graph.labels)
            
    # 将所有结果和标签合并成一个大张量
    all_labels_tensor = torch.cat(all_labels, dim=0)
    all_preds_tensor = torch.cat(all_preds, dim=0)
    
    # 计算并返回指标
    return calculate_metrics(all_labels_tensor, all_preds_tensor)
# main.py (REVISED)
import torch
import torch.optim as optim
import torch.nn.functional as F
import os

# 从其他模块导入必要的组件
from model import DFHNet
from train import train_model
from utils import FocalLoss

# ！！！关键改动 1: 导入 PyG 的 DataLoader 和 Dataset ！！！
from torch_geometric.data import Dataset
from torch_geometric.loader import DataLoader

class SyntheticDataset(Dataset):
    """
    加载 PyG Data 对象列表的 Dataset 类。
    """
    def __init__(self, data_path):
        super(SyntheticDataset, self).__init__()
        self.data = torch.load(data_path, weights_only=False)
        print(f"从 {data_path} 加载了 {len(self.data)} 个 PyG Data 对象。")
        
    def len(self):
        return len(self.data)
        
    def get(self, idx):
        return self.data[idx]

if __name__ == '__main__':
    # --- 1. 环境和参数设置 ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"--- 使用设备: {device} ---")

    # --- 2. 加载 GO 和模拟数据集 ---
    try:
        go_data = torch.load('data/go_data.pth', weights_only=False)
        go_adj_matrix = go_data['adj_matrix'].to(device)
        GO_TERM_COUNT = len(go_data['go_to_idx'])
    except FileNotFoundError:
        print("错误: data/go_data.pth 未找到。请先运行 prepare_go_dag.py。")
        exit()

    dataset_dir = "synthetic_dataset"
    train_path = os.path.join(dataset_dir, "train_dataset.pth")
    val_path = os.path.join(dataset_dir, "val_dataset.pth")
    if not os.path.exists(train_path):
        print(f"错误: 在 '{dataset_dir}' 文件夹中未找到数据集。请先运行 create_and_save_dataset.py。")
        exit()
        
    train_dataset = SyntheticDataset(train_path)
    val_dataset = SyntheticDataset(val_path)
    
    # ！！！关键改动 2: 使用 PyG 的 DataLoader ！！！
    # batch_size > 1 现在也可以正常工作了
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)
    
    # --- 3. 实例化模型、损失函数和优化器 ---
    model_params = {
        'num_domains': 5000, 'domain_embed_dim': 256, 'go_term_count': GO_TERM_COUNT,
        'agcn_in_dim': 1280, 'mcnn_in_dim': 1280, 'go_adj_matrix': go_adj_matrix,
        'protein_embed_dim': 1024
    }
    model = DFHNet(**model_params).to(device)
    
    criterion = FocalLoss(alpha=0.25, gamma=2.0)
    print(f"使用 {type(criterion).__name__} 作为损失函数。")
    
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    EPOCHS = 10
    
    # --- 4. 启动训练 (需要修改 train.py 来适应新的数据格式) ---
    print("\n--- 开始启动训练流程 ---")
    # 注意：您还需要对 train.py 和 evaluate.py 做微小调整
    train_model(
        model=model, train_loader=train_loader, val_loader=val_loader,
        optimizer=optimizer, criterion=criterion, epochs=EPOCHS, device=device
    )
    
    print("\n--- 训练完成！ ---")
# model.py (REVISED AND IMPROVED)
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool


# ===================================================================
# ==  原始模块 (保留 AGCN 和 MCNN 的基础结构)                      ==
# ===================================================================

class AGCN(nn.Module):
    """
    自适应图卷积网络 (AGCN) - 结构流
    - 保持基本 GCN 结构用于学习局部结构特征
    - 移除了 MHA 和 predictor，因为特征聚合和预测将在更高层次完成
    """

    def __init__(self, in_channels, out_channels, dropout=0.1):
        super(AGCN, self).__init__()
        # 建议增加残差连接以获得更好的性能，此处为简化暂未添加
        self.conv1 = GCNConv(in_channels, 512)
        self.conv2 = GCNConv(512, out_channels)  # 输出维度直接为蛋白质嵌入维度
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.relu(self.conv1(x, edge_index))
        x = self.dropout(x)
        x = self.relu(self.conv2(x, edge_index))
        x = self.dropout(x)
        # 输出每个残基的特征，供后续的注意力模块使用
        return x


class MCNN(nn.Module):
    """
    多层卷积神经网络 (MCNN) - 序列流
    - 保持基本 CNN 结构用于学习序列局部特征
    - 移除了 predictor，因为预测将在更高层次完成
    """

    def __init__(self, in_channels, out_channels, dropout=0.1):
        super(MCNN, self).__init__()
        self.conv1 = nn.Conv1d(in_channels, 512, kernel_size=5, padding=2)
        self.bn1 = nn.BatchNorm1d(512)
        self.conv2 = nn.Conv1d(512, out_channels, kernel_size=5, padding=2)
        self.bn2 = nn.BatchNorm1d(out_channels)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, data):
        # MCNN 操作的是序列特征: 期望 [batch, channels, length]
        x = data.x.unsqueeze(0).permute(0, 2, 1)  # [L, C] -> [1, C, L]
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.dropout(x)
        x = self.relu(self.bn2(self.conv2(x)))
        x_permuted = x.permute(0, 2, 1)  # [1, C, L] -> [1, L, C]
        # 输出每个残基的特征，供后续的注意力模块使用
        return x_permuted.squeeze(0)  # [L, C]


# ===================================================================
# ==  第一部分改进: 借鉴 DPFunc 的域引导注意力机制                 ==
# ===================================================================
class DomainAttention(nn.Module):
    """
    域引导的注意力模块.
    该模块利用域信息作为查询(Query)，计算每个残基的重要性，
    并对残基特征进行加权求和，生成一个全局的蛋白质嵌入。
    """

    def __init__(self, domain_embed_dim, residue_feature_dim, attention_dim):
        super().__init__()
        self.query_proj = nn.Linear(domain_embed_dim, attention_dim)
        self.key_proj = nn.Linear(residue_feature_dim, attention_dim)
        self.value_proj = nn.Linear(residue_feature_dim, residue_feature_dim)
        self.scale = attention_dim ** -0.5

    def forward(self, domain_embedding_sum, residue_features):
        # domain_embedding_sum: [1, domain_embed_dim]
        # residue_features: [L, residue_feature_dim]

        # 1. 将域嵌入和残基特征投影到注意力空间
        # Q: [1, attention_dim], K: [L, attention_dim], V: [L, residue_feature_dim]
        q = self.query_proj(domain_embedding_sum)
        k = self.key_proj(residue_features)
        v = self.value_proj(residue_features)

        # 2. 计算注意力得分
        # attention_scores: [1, L]
        attention_scores = torch.matmul(q, k.transpose(0, 1)) * self.scale
        attention_weights = F.softmax(attention_scores, dim=-1)  # [1, L]

        # 3. 对残基特征(V)进行加权求和
        # context_vector: [1, residue_feature_dim]
        context_vector = torch.matmul(attention_weights, v)

        return context_vector


# ===================================================================
# ==  第二部分改进: 借鉴 TAWFN 的双流自适应融合架构              ==
# ===================================================================
class DFHEncoder(nn.Module):
    """
    重构后的 DFH 编码器.
    包含 AGCN 和 MCNN 两个并行的流，每个流都使用域注意力来生成蛋白质嵌入，
    并独立进行初步预测。
    """

    def __init__(self, num_domains, domain_embed_dim, protein_embed_dim, go_term_count, agcn_in_dim, mcnn_in_dim):
        super().__init__()
        print("Initializing Re-architected DFHEncoder...")

        # 基础特征提取模块
        self.gcn_stream = AGCN(in_channels=agcn_in_dim, out_channels=protein_embed_dim)
        self.cnn_stream = MCNN(in_channels=mcnn_in_dim, out_channels=protein_embed_dim)

        # 域嵌入模块
        self.domain_embedding = nn.Embedding(num_domains, domain_embed_dim)

        # 域注意力模块 (两个流共享同一个注意力机制)
        self.attention = DomainAttention(domain_embed_dim, protein_embed_dim, attention_dim=256)

        # 每个流独立的预测器
        self.gcn_predictor = nn.Linear(protein_embed_dim, go_term_count)
        self.cnn_predictor = nn.Linear(protein_embed_dim, go_term_count)

    def forward(self, data, domain_ids):
        # 1. 获取所有域的嵌入并求和，作为全局域信息
        domain_embeds = self.domain_embedding(domain_ids)
        domain_embed_sum = torch.sum(domain_embeds, dim=0, keepdim=True)  # [1, domain_embed_dim]

        # 2. 并行处理 AGCN 和 MCNN 流
        gcn_residue_features = self.gcn_stream(data)  # [L, protein_embed_dim]
        cnn_residue_features = self.cnn_stream(data)  # [L, protein_embed_dim]

        # 3. 每个流都通过域注意力模块生成蛋白质嵌入
        gcn_protein_embedding = self.attention(domain_embed_sum, gcn_residue_features)
        cnn_protein_embedding = self.attention(domain_embed_sum, cnn_residue_features)

        # 4. 每个流都进行初步预测，输出 logits
        gcn_logits = self.gcn_predictor(gcn_protein_embedding)
        cnn_logits = self.cnn_predictor(cnn_protein_embedding)

        return gcn_logits, cnn_logits


class AdaptiveFusion(nn.Module):
    """
    自适应融合模块，学习如何加权融合来自两个流的预测结果。
    """

    def __init__(self):
        super().__init__()
        # 创建一个可学习的参数 alpha，用于控制融合权重
        # 初始化为0.5，表示初始时两个流同等重要
        self.alpha = nn.Parameter(torch.tensor(0.5))

    def forward(self, gcn_logits, cnn_logits):
        # 使用 sigmoid 保证 alpha 在 0-1 之间
        fused_logits = torch.sigmoid(self.alpha) * gcn_logits + (1 - torch.sigmoid(self.alpha)) * cnn_logits
        return fused_logits


# ===================================================================
# ==  保留的创新点: 层次化解码器                                   ==
# ===================================================================
class HierarchicalDecoder(nn.Module):
    def __init__(self, go_adj_matrix):
        super().__init__()
        print("Initializing Hierarchical Decoder...")
        # 解码器现在直接作用于融合后的 logits，不再需要自己的 predictor
        self.register_buffer('go_adj_matrix', go_adj_matrix)

    def forward(self, fused_logits, iterations=2):
        # True Path Rule: 如果子节点被预测，那么父节点也必须被预测
        # P(parent) >= max(P(children))

        current_logits = fused_logits
        # adj_matrix[i, j] = 1 表示 j 是 i 的父节点
        # 我们需要从子节点传播分数到父节点

        for _ in range(iterations):
            # 将 logits 转换为概率，以便进行 max 操作
            current_probs = torch.sigmoid(current_logits)

            # 找到每个节点的所有子节点分数的最大值
            # torch.matmul(A.T, p) 会聚合所有子节点的分数给父节点
            # A.T 的维度是 [num_terms, num_terms], current_probs 是 [1, num_terms]
            # 我们需要让 A[i,j] 表示 i 是 j 的父节点，这样 A.T[j,i] 表示 j 是 i 的子节点
            # 因此，我们需要 adj_matrix 本身

            # parent_probs_from_children = torch.matmul(current_probs, self.go_adj_matrix) -> 这样是子节点聚合到父节点
            # A[i, j]=1 (j is parent of i) -> P_i should be affected by P_j
            # We want P_j = max(P_j, P_i) for all children i

            # Let's rebuild the logic. For each term j, find its children i. P_j = max(P_j, P_i1, P_i2...)
            # This is equivalent to propagating child probabilities upwards.
            # `child_probs_agg[j] = sum_{i where j is parent of i} P_i` if using matmul(P, A)
            # What we need is max propagation.

            # 使用转置矩阵，A_T[j, i] = 1 表示 i 是 j 的父节点
            # max_child_logits, _ = torch.max(current_logits * self.go_adj_matrix.T, dim=1, keepdim=True)
            # This logic is complex with sparse matrices. Let's stick to the original logic which is sound.
            # A[i,j]=1 means j is parent of i. matmul(probs, A) -> for each parent j, aggregate scores from children i.

            agg_child_probs = torch.matmul(current_probs, self.go_adj_matrix)

            # 更新概率：如果任何一个子节点的概率高于父节点，则提升父节点的概率
            # P_new = max(P_current, P_aggregated_from_children)
            updated_probs = torch.max(current_probs, agg_child_probs)

            # 将更新后的概率转换回 logits，用于下一次迭代或最终输出
            # 防止概率为1或0导致log无限大
            updated_probs = torch.clamp(updated_probs, 1e-7, 1 - 1e-7)
            current_logits = torch.log(updated_probs / (1 - updated_probs))

        final_logits = current_logits
        final_probabilities = torch.sigmoid(final_logits)

        return final_probabilities, final_logits


# ===================================================================
# ==  最终组装的、大幅改进的 DFHNet 模型                           ==
# ===================================================================
class DFHNet(nn.Module):
    def __init__(self, num_domains, domain_embed_dim, go_term_count, agcn_in_dim, mcnn_in_dim, go_adj_matrix,
                 protein_embed_dim):
        super().__init__()
        print("Assembling the final IMPROVED DFHNet model...")
        self.encoder = DFHEncoder(num_domains, domain_embed_dim, protein_embed_dim, go_term_count, agcn_in_dim,
                                  mcnn_in_dim)
        self.fusion = AdaptiveFusion()
        self.decoder = HierarchicalDecoder(go_adj_matrix=go_adj_matrix)

    def forward(self, data, domain_ids):
        # 1. 编码器输出两个独立的初步预测
        gcn_logits, cnn_logits = self.encoder(data, domain_ids)

        # 2. 自适应融合模块对预测结果进行加权融合
        fused_logits = self.fusion(gcn_logits, cnn_logits)

        # 3. 层次化解码器对融合后的结果进行最终优化
        final_probs, final_logits = self.decoder(fused_logits)

        return final_probs, final_logits
# prepare_go_dag.py
import torch
import obonet
import networkx as nx
import numpy as np


def prepare_go_data(go_file_path='data/go.obo', output_path='data/go_data.pth'):
    """
    读取 go.obo 文件, 创建 GO term 的邻接矩阵和索引映射.
    """
    print("Loading GO graph from obo file...")
    graph = obonet.read_obo(go_file_path)

    # 筛选出三个主要分支: BP, MF, CC
    valid_namespaces = {'biological_process', 'molecular_function', 'cellular_component'}
    valid_nodes = [node for node, data in graph.nodes(data=True) if data.get('namespace') in valid_namespaces]

    # 创建一个只包含有效节点的子图
    go_dag = graph.subgraph(valid_nodes)

    # 获取所有 GO terms 并创建一个从 GO ID 到整数索引的映射
    go_terms = sorted(list(go_dag.nodes()))
    go_to_idx = {term: i for i, term in enumerate(go_terms)}
    num_go_terms = len(go_terms)
    print(f"Found {num_go_terms} valid GO terms.")

    # 创建邻接矩阵 A, A[i, j] = 1 表示 GO term j 是 GO term i 的父节点
    adj_matrix = np.zeros((num_go_terms, num_go_terms), dtype=np.float32)

    for term, idx in go_to_idx.items():
        # networkx中, successor 是子节点, predecessor 是父节点
        # 我们要找父节点,所以用 predecessors
        for parent in go_dag.predecessors(term):
            if parent in go_to_idx:
                parent_idx = go_to_idx[parent]
                # 在我们的定义中, adj_matrix[i,j]=1表示j是i的父节点
                adj_matrix[idx, parent_idx] = 1

    # 将Numpy矩阵转换为PyTorch Tensor
    adj_matrix_tensor = torch.from_numpy(adj_matrix)

    # 保存邻接矩阵和索引映射
    torch.save({
        'adj_matrix': adj_matrix_tensor,
        'go_to_idx': go_to_idx,
        'idx_to_go': {i: term for term, i in go_to_idx.items()}
    }, output_path)

    print(f"GO data saved to {output_path}")


if __name__ == '__main__':
    prepare_go_data()
# train.py
import torch
from evaluate import evaluate_model
from tqdm import tqdm

def train_model(model, train_loader, val_loader, optimizer, criterion, epochs, device):
    """
    模型训练的主循环。
    此版本已更新，可正确处理 PyTorch Geometric 的批次数据。
    """
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        
        # 使用 tqdm 显示带有描述的进度条
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{epochs} [Training]")
        for batch_data in train_pbar:
            # 1. 将整个批次数据一次性移动到 GPU
            batch_data = batch_data.to(device)
            optimizer.zero_grad()

            # 2. 将一个 Batch 对象分解成一个包含单个 Data 对象的 Python 列表
            graph_list = batch_data.to_data_list()
            
            # 用于存储批次内每个样本的输出和标签
            batch_probs = []
            batch_labels = []

            # 3. 逐个处理批次中的每个图（因为模型当前 forward 函数设计为处理单个图）
            for graph in graph_list:
                probs, _ = model(graph, graph.domains)
                batch_probs.append(probs)
                batch_labels.append(graph.labels)
            
            # 4. 将该批次所有样本的结果和标签重新合并成一个张量
            #    例如，如果 batch_size=8, final_probs 的维度将是 [8, num_classes]
            final_probs = torch.cat(batch_probs, dim=0)
            final_labels = torch.cat(batch_labels, dim=0)

            # 5. 计算损失，此时维度匹配，不会报错
            loss = criterion(final_probs, final_labels)
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            # 在进度条后面显示当前批次的损失
            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})
            
        avg_loss = total_loss / len(train_loader)
        alpha = torch.sigmoid(model.fusion.alpha).item()
        
        # --- 每个 epoch 结束后进行验证 ---
        fmax, macro_aupr = evaluate_model(model, val_loader, device)
        
        # 打印当前周期的完整信息
        print(f"\nEpoch {epoch + 1}/{epochs} | "
              f"训练损失: {avg_loss:.4f} | "
              f"GCN权重(alpha): {alpha:.4f} | "
              f"验证 Fmax: {fmax:.4f} | "
              f"验证 Macro AUPR: {macro_aupr:.4f}")
import torch
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from sklearn.metrics import precision_recall_curve, auc
import os

# 从我们修改好的模型文件中导入 DFHNet
from model import DFHNet


def calculate_metrics(y_true, y_pred_probs):
    """
    计算 Fmax 和 AUPR 指标。
    - y_true: 真实的标签矩阵 [num_samples, num_go_terms]
    - y_pred_probs: 模型预测的概率矩阵 [num_samples, num_go_terms]
    """
    y_true = y_true.numpy()
    y_pred_probs = y_pred_probs.numpy()

    # --- 计算 Fmax ---
    fmax = 0.0
    for threshold in np.linspace(0, 1, 101):
        precision_total, recall_total = 0, 0
        num_proteins_with_preds = 0

        for i in range(y_true.shape[0]):
            preds = y_pred_probs[i] > threshold
            true_positives = np.sum(preds & y_true[i].astype(bool))

            if np.sum(preds) > 0:
                num_proteins_with_preds += 1
                precision_total += true_positives / np.sum(preds)

            if np.sum(y_true[i]) > 0:
                recall_total += true_positives / np.sum(y_true[i])

        if num_proteins_with_preds > 0:
            avg_precision = precision_total / num_proteins_with_preds
            avg_recall = recall_total / y_true.shape[0]

            if avg_precision + avg_recall > 0:
                f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall)
                if f1 > fmax:
                    fmax = f1

    # --- 计算 AUPR (Macro Average) ---
    aupr_scores = []
    for i in range(y_true.shape[1]):  # 遍历每个GO term
        if np.sum(y_true[:, i]) > 0:  # 只计算至少有一个正样本的类别
            precision, recall, _ = precision_recall_curve(y_true[:, i], y_pred_probs[:, i])
            aupr_scores.append(auc(recall, precision))

    macro_aupr = np.mean(aupr_scores) if aupr_scores else 0.0

    return fmax, macro_aupr


if __name__ == '__main__':
    print("--- 1. 开始加载模型和数据 ---")

    # --- 加载 GO 数据 ---
    try:
        go_data = torch.load('data/go_data.pth')
        go_adj_matrix = go_data['adj_matrix']
        GO_TERM_COUNT = len(go_data['go_to_idx'])
    except FileNotFoundError:
        print("错误: data/go_data.pth 未找到。请先运行 prepare_go_dag.py。")
        exit()

    # --- 加载模拟数据集 ---
    dataset_dir = "synthetic_dataset"
    train_path = os.path.join(dataset_dir, "train_dataset.pth")
    val_path = os.path.join(dataset_dir, "val_dataset.pth")

    try:
        train_dataset = torch.load(train_path)
        val_dataset = torch.load(val_path)
        print("模拟数据集加载成功。")
    except FileNotFoundError:
        print(f"错误: 在 '{dataset_dir}' 文件夹中未找到数据集。请先运行 create_and_save_dataset.py。")
        exit()

    # --- 模型参数 ---
    NUM_DOMAINS = 5000
    DOMAIN_EMBED_DIM = 256
    FEATURE_DIM = 1280
    PROTEIN_EMBED_DIM = 1024
    EPOCHS = 10

    # --- 实例化模型、损失函数和优化器 ---
    model = DFHNet(
        num_domains=NUM_DOMAINS, domain_embed_dim=DOMAIN_EMBED_DIM, go_term_count=GO_TERM_COUNT,
        agcn_in_dim=FEATURE_DIM, mcnn_in_dim=FEATURE_DIM, go_adj_matrix=go_adj_matrix,
        protein_embed_dim=PROTEIN_EMBED_DIM
    )
    # 推荐使用 FocalLoss，但为保持脚本简洁性，此处使用标准的 BCE Loss
    criterion = F.binary_cross_entropy
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # --- 2. 开始训练 ---
    print("\n--- 2. 开始训练 ---")
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        for sample in train_dataset:
            optimizer.zero_grad()
            probs, _ = model(sample["graph"], sample["domains"])
            loss = criterion(probs.squeeze(0), sample["labels"])
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(train_dataset)
        alpha = torch.sigmoid(model.fusion.alpha).item()
        print(f"Epoch {epoch + 1}/{EPOCHS} | 平均损失: {avg_loss:.4f} | GCN权重(alpha): {alpha:.4f}")

    # --- 3. 开始验证和评估 ---
    print("\n--- 3. 开始验证和评估 ---")
    model.eval()
    all_labels = []
    all_preds = []
    with torch.no_grad():
        for sample in val_dataset:
            probs, _ = model(sample["graph"], sample["domains"])
            all_labels.append(sample["labels"])
            all_preds.append(probs.squeeze(0))

    all_labels_tensor = torch.stack(all_labels)
    all_preds_tensor = torch.stack(all_preds)

    fmax, macro_aupr = calculate_metrics(all_labels_tensor, all_preds_tensor)

    print("\n--- 4. 评估结果 ---")
    print(f"Fmax: {fmax:.4f}")
    print(f"Macro AUPR: {macro_aupr:.4f}")
    print("\n验证完成！")
# utils.py
import torch
import torch.nn as nn
import torch.nn.functional as F


class FocalLoss(nn.Module):
    """
    Focal Loss, 用于解决多标签分类中的类别不平衡问题。
    """

    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        bce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-bce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss

        if self.reduction == 'mean':
            return torch.mean(focal_loss)
        elif self.reduction == 'sum':
            return torch.sum(focal_loss)
        else:
            return focal_loss
